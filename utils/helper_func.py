import os
import re
import time
import datetime
from typing import  List, Set, Tuple, Dict
import ast


from tqdm import tqdm 
import pandas as pd
import requests
import numpy as np
import os
import math

# é å”®å±‹æŸ¥è©¢ - ç¶²å€çµ„åˆfunction
def build_complete_urls(base_url, url_fragments):
    """å°‡base URLèˆ‡fragmentsçµ„åˆæˆå®Œæ•´URLå­—å…¸
    
    Args:
        base_url (str): åŸºç¤URL
        url_fragments (dict): URLç‰‡æ®µå­—å…¸ï¼Œkeyç‚ºåŸå¸‚åç¨±ï¼Œvalueç‚ºURLç‰‡æ®µ
        
    Returns:
        dict: å®Œæ•´URLå­—å…¸, keyç‚ºåŸå¸‚åç¨±, valueç‚ºå®Œæ•´URL
    """
    return {city: base_url + fragment for city, fragment in url_fragments.items()}

def fetch_data(url):
    try:
        response = requests.get(url)
        response.raise_for_status()  # è‹¥æœ‰éŒ¯èª¤ç‹€æ³ï¼Œæœƒå¼•ç™¼ä¾‹å¤–
        data = response.json()
        return pd.DataFrame(data)
    except Exception as e:
        print(f"å–å¾—è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        return pd.DataFrame()  # å›å‚³ç©ºçš„ DataFrame
    
# å–å‡ºå»ºå•†
def extract_company_name(row):
    """
    æ¯”å°idèˆ‡idlistä¸­çš„IDï¼Œæå–åŒ¹é…çš„å…¬å¸åç¨±
    """
    target_id = row['ç·¨è™Ÿ']
    idlist = row['ç·¨è™Ÿåˆ—è¡¨']
    
    # å¦‚æœidlistæ˜¯å­—ä¸²æ ¼å¼çš„åˆ—è¡¨ï¼Œéœ€è¦å…ˆè½‰æ›
    if isinstance(idlist, str):
        try:
            # å˜—è©¦ç”¨ast.literal_evalè½‰æ›å­—ä¸²æ ¼å¼çš„åˆ—è¡¨
            idlist = ast.literal_eval(idlist)
        except:
            # å¦‚æœè½‰æ›å¤±æ•—ï¼Œè¿”å›None
            return None
    
    # éæ­·idlistä¸­çš„æ¯å€‹é …ç›®
    for item in idlist:
        # æŒ‰é€—è™Ÿåˆ†å‰²å­—ä¸²
        parts = item.split(',')
        if len(parts) >= 3:  # ç¢ºä¿æœ‰è¶³å¤ çš„éƒ¨åˆ†
            item_id = parts[0].strip()  # IDéƒ¨åˆ†
            company_name = parts[2].strip()  # å…¬å¸åç¨±éƒ¨åˆ†
            
            # æ¯”å°ID
            if item_id == target_id:
                return company_name
    
    # å¦‚æœæ²’æœ‰æ‰¾åˆ°åŒ¹é…çš„IDï¼Œè¿”å›None
    return None


# åˆä½µdataframe
def combined_df(url, input_time):
    # å»ºç«‹ä¸€å€‹ç©ºçš„åˆ—è¡¨å­˜æ”¾å„å€çš„ DataFrame
    df_list = []
    city_counts = {}  # ç”¨æ–¼è¨˜éŒ„æ¯å€‹ç¸£å¸‚çš„è³‡æ–™ç­†æ•¸
    
    print("é–‹å§‹è™•ç†å„ç¸£å¸‚è³‡æ–™ï¼š")
    
    # è¿´åœˆèµ°è¨ªæ‰€æœ‰ URLï¼Œä¸¦æ–°å¢è¡¨ç¤ºåœ°å€å’Œè¼¸å…¥æ™‚é–“çš„æ¬„ä½
    for city_name, uni_url in url.items():
        print(f"è™•ç† {city_name} ä¸­...", end="", flush=True)
        
        df_temp = fetch_data(uni_url)
        if not df_temp.empty:
            df_temp["city_name"] = city_name      # åŠ å…¥ä¾†æºå€åŸŸæ¬„ä½ï¼Œä¾¿æ–¼å¾ŒçºŒåˆ†æ
            df_temp["input_time"] = input_time    # åŠ å…¥å¾è®Šæ•¸åç¨±æå–çš„æ™‚é–“

            # ç«‹åˆ»é‡æ’æ¬„ä½ï¼Œè®“ city_name æˆç‚ºç¬¬ä¸€æ¬„
            cols = df_temp.columns.tolist()
            cols = ["city_name"] + [c for c in cols if c != "city_name"]
            df_temp = df_temp[cols]
            
            # è¨˜éŒ„æ­¤ç¸£å¸‚çš„è³‡æ–™ç­†æ•¸
            row_count = len(df_temp)
            city_counts[city_name] = row_count
            # ç›´æ¥æ‰“å°ç•¶å‰ç¸£å¸‚çš„è³‡æ–™ç­†æ•¸
            print(f" å®Œæˆ! æ‰¾åˆ° {row_count} ç­†è³‡æ–™")
        else:
            print(" å®Œæˆ! æ‰¾åˆ° 0 ç­†è³‡æ–™")
            
        df_list.append(df_temp)
        time.sleep(1)  # æ¯æ¬¡ç™¼å‡ºè«‹æ±‚å¾Œæš«åœ 1 ç§’

    # åˆ©ç”¨ pd.concat åˆä½µæ‰€æœ‰ DataFrameï¼ˆé‡ç½®ç´¢å¼•ï¼‰
    combined_df = pd.concat(df_list, ignore_index=True)
    
    # # é¡¯ç¤ºå„ç¸£å¸‚è³‡æ–™ç­†æ•¸çµ±è¨ˆ
    # print("\nå„ç¸£å¸‚è³‡æ–™ç­†æ•¸çµ±è¨ˆ:")
    # for city, count in city_counts.items():
    #     print(f"{city}: {count} ç­†")
    
    # é¡¯ç¤ºåˆä½µå¾Œçš„ç¸½ç­†æ•¸
    total_rows = len(combined_df)
    print(f"\nåˆä½µå¾Œè³‡æ–™ç¸½ç­†æ•¸: {total_rows} ç­†")
    
    return combined_df


# ç”±åè½åœ°å€æ¬„ä½æŠ˜åˆ†å‡ºè¡Œæ”¿å€
def parse_admin_region(address):
    # è‹¥ä¸æ˜¯å­—ä¸²æˆ–å­—ä¸²é•·åº¦ç‚º0ï¼Œç›´æ¥å›å‚³ None æˆ–ç©ºå€¼
    if not isinstance(address, str) or not address:
        return None
    
    # åˆ¤æ–·ç¬¬äºŒå€‹å­—æ˜¯å¦ç‚ºã€Œå€ã€
    # æ³¨æ„ï¼šPython å­—ä¸²çš„ç´¢å¼•å¾ 0 é–‹å§‹
    if len(address) >= 2 and address[1] == "å€":
        return address[:2]
    # åˆ¤æ–·ç¬¬ä¸‰å€‹å­—æ˜¯å¦ç‚ºã€Œå€ã€
    elif len(address) >= 3 and address[2] == "å€":
        return address[:3]
    # å…¶é¤˜æƒ…æ³å–å‰ä¸‰å€‹å­—
    elif len(address) >= 3:
        return address[:3]
    else:
        # è‹¥å­—ä¸²ä¸è¶³ä¸‰å€‹å­—ï¼Œå°±ç›´æ¥å›å‚³åŸå­—ä¸²
        return address
    

# å®šç¾©ä¸€å€‹å‡½å¼ä¾†è§£æéŠ·å”®æœŸé–“ï¼Œå›å‚³ (è‡ªå”®æœŸé–“, ä»£éŠ·æœŸé–“)
def parse_sale_period(s: str) -> Tuple[str, str]:
    """
    è§£æã€ŒéŠ·å”®æœŸé–“ã€å­—ä¸²ï¼Œå›å‚³ (è‡ªå”®æ™‚æ®µ, ä»£éŠ·æ™‚æ®µ)ã€‚

    è¦å‰‡ï¼š
      1. æ”¯æ´åˆ†éš”ç¬¦è™Ÿï¼šé€—è™Ÿ(,), é “è™Ÿ(ï¼Œ), åˆ†è™Ÿ(;)ã€‚
      2. è‹¥æ®µè½ä¸­å«ã€Œè‡ªå”®ã€é—œéµå­—ï¼Œè¦–ç‚ºè‡ªå”®æ™‚æ®µï¼›å«ã€Œä»£éŠ·ã€é—œéµå­—ï¼Œè¦–ç‚ºä»£éŠ·æ™‚æ®µã€‚
      3. è‹¥ç„¡æ˜ç¢ºé—œéµå­—ï¼Œå‰‡ç¬¬ä¸€æ®µçµ¦è‡ªå”®ã€ç¬¬äºŒæ®µçµ¦ä»£éŠ·ã€‚
      4. ã€Œç„¡ã€æˆ–ç©ºå­—ä¸²çš†å›å‚³ç©ºå­—ä¸²ã€‚
      5. ä¿ç•™åŸå§‹æ ¼å¼ï¼Œä¸åšæ—¥æœŸé©—è­‰ã€‚

    ç¯„ä¾‹ï¼š
      >>> parse_sale_period("è‡ªå”®:1100701~è‡ªå®Œå”®æ­¢;ä»£éŠ·:1100801~å”®å®Œç‚ºæ­¢")
      ("1100701~è‡ªå®Œå”®æ­¢", "1100801~å”®å®Œç‚ºæ­¢")
      >>> parse_sale_period("1100701~1110101,1120201~1120501")
      ("1100701~1110101", "1120201~1120501")
      >>> parse_sale_period("ç„¡")
      ("", "")
    """
    # ç©ºå€¼æˆ–ã€Œç„¡ã€ç›´æ¥å›ç©ºå­—ä¸²
    if not isinstance(s, str) or s.strip() in {"", "ç„¡"}:
        return "", ""

    # çµ±ä¸€åˆ†éš”ç¬¦ï¼Œä¸¦åˆ‡æ®µ
    normalized = re.sub(r"[ï¼›;ï¼Œ]", ",", s.strip())
    parts = [p.strip() for p in normalized.split(",") if p.strip()]

    self_period = ""
    agent_period = ""

    for part in parts:
        low = part.lower()
        # æ˜ç¢ºæ¨™ç¤ºã€Œè‡ªå”®ã€
        if "è‡ªå”®" in low:
            # å»æ‰æ¨™ç±¤ï¼Œä¿ç•™å¾Œé¢æ‰€æœ‰
            self_period = re.sub(r"(?i).*?è‡ªå”®[:ï¼š]?", "", part).strip()
        # æ˜ç¢ºæ¨™ç¤ºã€Œä»£éŠ·ã€
        elif "ä»£éŠ·" in low:
            agent_period = re.sub(r"(?i).*?ä»£éŠ·[:ï¼š]?", "", part).strip()
        else:
            # ç„¡æ¨™ç±¤ï¼Œä¾åºå¡«å…¥
            if not self_period:
                self_period = part
            elif not agent_period:
                agent_period = part
    return self_period, agent_period


# å®šç¾©å‡½å¼ï¼šå°‹æ‰¾è‡ªå”®æœŸé–“åŠä»£éŠ·æœŸé–“çš„èµ·å§‹æ—¥ï¼Œè‹¥æ²’æœ‰å‰‡å›å‚³ None
def find_first_sale_time(text):
    """
    å°‹æ‰¾è‡ªå”®æœŸé–“åŠä»£éŠ·æœŸé–“çš„èµ·å§‹æ—¥ï¼Œæ”¯æ´å¤šç¨®æ—¥æœŸæ ¼å¼
    
    æ”¯æ´æ ¼å¼ï¼š
    - 1110701 (7ä½æ•¸å­—)
    - 111å¹´07æœˆ01æ—¥ / 111å¹´7æœˆ1æ—¥ / 111å¹´8æœˆ1è™Ÿ
    - 111/07/01 / 111/7/1
    
    Returns:
        str: æ¨™æº–åŒ–çš„7ä½æ•¸å­—æ—¥æœŸæ ¼å¼ (å¦‚: '1110701')ï¼Œè‹¥æ²’æœ‰æ‰¾åˆ°å‰‡å›å‚³ None
    """
    if not isinstance(text, str):
        return None
    
    # 1. å…ˆæª¢æŸ¥æ˜¯å¦å·²ç¶“æ˜¯7ä½æ•¸å­—æ ¼å¼
    seven_digit_match = re.search(r"\d{7}", text)
    if seven_digit_match:
        return seven_digit_match.group(0)
    
    # 2. æª¢æŸ¥ "111å¹´07æœˆ01æ—¥"ã€"111å¹´7æœˆ1æ—¥" æˆ– "111å¹´8æœˆ1è™Ÿ" æ ¼å¼
    year_month_day_match = re.search(r"(\d{3})å¹´(\d{1,2})æœˆ(\d{1,2})[æ—¥è™Ÿ]", text)
    if year_month_day_match:
        year = year_month_day_match.group(1)
        month = year_month_day_match.group(2).zfill(2)  # è£œé›¶åˆ°2ä½
        day = year_month_day_match.group(3).zfill(2)    # è£œé›¶åˆ°2ä½
        return f"{year}{month}{day}"
    
    # 3. æª¢æŸ¥ "111/07/01" æˆ– "111/7/1" æ ¼å¼
    slash_format_match = re.search(r"(\d{3})/(\d{1,2})/(\d{1,2})", text)
    if slash_format_match:
        year = slash_format_match.group(1)
        month = slash_format_match.group(2).zfill(2)    # è£œé›¶åˆ°2ä½
        day = slash_format_match.group(3).zfill(2)      # è£œé›¶åˆ°2ä½
        return f"{year}{month}{day}"
    
    # 4. å¦‚æœéƒ½æ²’æœ‰åŒ¹é…ï¼Œå›å‚³ None
    return None


# å®šç¾©å‡½å¼ï¼šä¾æ“šè¦å‰‡æ±ºå®šå»ºæ¡ˆçš„ã€ŒéŠ·å”®èµ·å§‹æ™‚é–“ã€
def sales_start_time(row) -> str:
    """
    æ ¹æ“šä»¥ä¸‹è¦å‰‡æ±ºå®šã€ŒéŠ·å”®èµ·å§‹æ™‚é–“ã€ï¼š
    
    1. å¦‚æœåªæœ‰è‡ªå”®æˆ–åªæœ‰ä»£éŠ·æœ‰å€¼ï¼Œå°±å–æœ‰å€¼çš„é‚£å€‹ã€‚
    2. å¦‚æœå…©è€…çš†æœ‰å€¼ï¼Œè½‰ç‚ºæ•´æ•¸å¾Œæ¯”è¼ƒï¼Œå–æœ€å°è€…ï¼ˆå†è½‰å›å­—ä¸²ï¼‰ã€‚
    3. å¦‚æœè‡ªå”®å’Œä»£éŠ·éƒ½ç©ºï¼š
       3.1. è‹¥å‚™æŸ¥å®Œæˆæ—¥æœŸæœ‰å€¼ï¼Œå°±å›å‚³å‚™æŸ¥å®Œæˆæ—¥æœŸï¼›
       3.2. å¦å‰‡å›å‚³å»ºç…§æ ¸ç™¼æ—¥ã€‚
    4. å…¶ä»–æƒ…æ³ï¼ˆå¦‚æ ¼å¼éŒ¯èª¤ç­‰ï¼‰å›å‚³ç©ºå­—ä¸²ã€‚
    """
    self_time  = row.get("è‡ªå”®èµ·å§‹æ™‚é–“")
    agent_time = row.get("ä»£éŠ·èµ·å§‹æ™‚é–“")
    check_date = row.get("å‚™æŸ¥å®Œæˆæ—¥æœŸ")
    permit_date= row.get("å»ºç…§æ ¸ç™¼æ—¥")

    # 1. åªæœ‰ä¸€æ–¹æœ‰å€¼
    if pd.isna(self_time) and pd.notna(agent_time):
        return str(agent_time).strip()
    if pd.isna(agent_time) and pd.notna(self_time):
        return str(self_time).strip()

    # 2. å…©è€…çš†æœ‰å€¼ï¼Œå–è¼ƒæ—©(æ•¸å€¼æœ€å°)
    if pd.notna(self_time) and pd.notna(agent_time):
        try:
            self_val  = int(str(self_time).strip())
            agent_val = int(str(agent_time).strip())
            return str(min(self_val, agent_val))
        except ValueError:
            # è‹¥æ ¼å¼ç„¡æ³•è½‰æ•´æ•¸ï¼Œå›ç©º
            return ""

    # 3. è‡ªå”®èˆ‡ä»£éŠ·çš†ç©º
    if pd.isna(self_time) and pd.isna(agent_time):
        # 3.1 å‚™æŸ¥å®Œæˆæ—¥æœŸå„ªå…ˆ
        if pd.notna(check_date):
            return str(check_date).strip()
        # 3.2 å¦å‰‡å»ºç…§æ ¸ç™¼æ—¥
        if pd.notna(permit_date):
            return str(permit_date).strip()
        # éƒ½ç„¡å€¼
        return ""

    # 4. å…¶ä»–ç‹€æ³
    return ""
    

def to_year_quarter(ts) -> str:
    if pd.isna(ts):
        return ""
    s = str(ts).strip()
    if len(s) < 5:
        return ""
    year = s[:3]
    try:
        month = int(s[3:5])
    except ValueError:
        return ""
    quarter = (month - 1) // 3 + 1
    if quarter < 1 or quarter > 4:
        return ""
    return f"{year}Y{quarter}S"


# å°‡åŸå§‹csvæª”æ‹†æˆå°æª”æ¡ˆ
def sample_csv_to_target_size(
    input_path,
    output_path="sample_10mb.csv",
    target_mb=10,
    encoding="utf-8-sig",
    random_state=42
):
    """
    å¾å¤§å‹ CSV ä¸­æŠ½æ¨£ï¼Œç”¢å‡ºæ¥è¿‘æŒ‡å®š MB å¤§å°çš„ CSV æª”æ¡ˆã€‚
    
    Parameters:
    - input_path: åŸå§‹ CSV è·¯å¾‘
    - output_path: æŠ½æ¨£å¾Œå„²å­˜çš„ CSV è·¯å¾‘
    - target_mb: ç›®æ¨™å¤§å°ï¼ˆä»¥ MB ç‚ºå–®ä½ï¼Œé è¨­ 10MBï¼‰
    - encoding: è¼¸å‡ºæª”æ¡ˆçš„ç·¨ç¢¼ï¼ˆé è¨­ç‚º utf-8-sigï¼‰
    - random_state: æŠ½æ¨£äº‚æ•¸ç¨®å­ï¼ˆç¢ºä¿å¯é‡ç¾ï¼‰
    """
    # è®€å–åŸå§‹ CSV
    print(f" è®€å–æª”æ¡ˆä¸­ï¼š{input_path}")
    df = pd.read_csv(input_path)

    # è¨ˆç®—å¹³å‡æ¯åˆ—å¤§å°
    avg_row_size = df.memory_usage(deep=True).sum() / len(df)
    target_bytes = target_mb * 1024 * 1024
    target_rows = int(target_bytes / avg_row_size)

    print(f" å¹³å‡æ¯åˆ—å¤§å°ï¼šç´„ {avg_row_size:.2f} bytes")
    print(f" ç›®æ¨™å¤§å°ï¼š{target_mb}MB â‰ˆ {target_rows} ç­†è³‡æ–™")

    # éš¨æ©ŸæŠ½æ¨£
    sampled_df = df.sample(n=target_rows, random_state=random_state)

    # å„²å­˜çµæœ
    sampled_df.to_csv(output_path, index=False, encoding=encoding)
    actual_size = os.path.getsize(output_path) / (1024 * 1024)

    print(f" å·²å„²å­˜æª”æ¡ˆï¼š{output_path}ï¼Œå¯¦éš›å¤§å°ç´„ {actual_size:.2f} MB")
    return sampled_df





# å‹æ…‹è½‰æˆdatetime64
def convert_mixed_date_columns(df, roc_cols=[], ad_cols=[], roc_slash_cols=[]):
    def parse_roc_integer(val):
        if pd.isna(val): return pd.NaT
        try:
            val = str(int(val)).zfill(7)
            y, m, d = int(val[:3]) + 1911, int(val[3:5]), int(val[5:7])
            return pd.Timestamp(f"{y}-{m:02d}-{d:02d}")
        except: return pd.NaT

    def parse_ad_integer(val):
        if pd.isna(val): return pd.NaT
        try:
            val = str(int(val)).zfill(8)
            return pd.to_datetime(val, format="%Y%m%d", errors='coerce')
        except: return pd.NaT

    def parse_roc_slash(val):
        if pd.isna(val): return pd.NaT
        try:
            parts = str(val).split('/')
            y, m, d = int(parts[0]) + 1911, int(parts[1]), int(parts[2])
            return pd.Timestamp(f"{y}-{m:02d}-{d:02d}")
        except: return pd.NaT

    for col in roc_cols:
        df[col] = df[col].apply(parse_roc_integer)

    for col in ad_cols:
        df[col] = df[col].apply(parse_ad_integer)

    for col in roc_slash_cols:
        df[col] = df[col].apply(parse_roc_slash)

    return df


# community_dfå–å‡ºç·¨è™Ÿåˆ—è¡¨ä¸­æ‰€æœ‰çš„å‚™æŸ¥ç·¨è™Ÿ
def extract_mixed_alphanumeric_ids(text):
    if pd.isna(text):
        return ''
    ids = re.findall(r'\b[A-Z0-9]{10,16}\b', text)
    return ', '.join(ids)


def parse_id_string(text: str) -> List[str]:
    """
    è§£æå‚™æŸ¥ç·¨è™Ÿæ¸…å–®å­—ä¸²ï¼Œè¿”å›æ¸…ç†å¾Œçš„ç·¨è™Ÿåˆ—è¡¨
    
    Parameters:
    -----------
    text : str
        åŒ…å«å¤šå€‹å‚™æŸ¥ç·¨è™Ÿçš„å­—ä¸²ï¼Œä»¥é€—è™Ÿåˆ†éš”
        
    Returns:
    --------
    List[str]
        æ¸…ç†å¾Œçš„å‚™æŸ¥ç·¨è™Ÿåˆ—è¡¨
    """
    if pd.isna(text) or not isinstance(text, str) or text.strip() == '':
        return []
    
    return [s.strip().strip("'\"") for s in text.split(',') if s.strip()]


def create_transaction_lookup_structures(transaction_df: pd.DataFrame) -> Tuple[Dict[str, int], pd.Series]:
    """
    å»ºç«‹äº¤æ˜“è³‡æ–™çš„æŸ¥è©¢çµæ§‹ï¼Œå„ªå…ˆåŸºæ–¼å‚™æŸ¥ç·¨è™Ÿçµ±è¨ˆ
    
    Parameters:
    -----------
    transaction_df : pd.DataFrame
        äº¤æ˜“è³‡æ–™ï¼Œéœ€åŒ…å«å‚™æŸ¥ç·¨è™Ÿã€ç¸£å¸‚ã€è¡Œæ”¿å€ã€ç¤¾å€åç¨±æ¬„ä½
        
    Returns:
    --------
    Tuple[Dict[str, int], pd.Series]
        (å‚™æŸ¥ç·¨è™Ÿå°æ‡‰çš„äº¤æ˜“ç­†æ•¸, composite_keyå°æ‡‰çš„äº¤æ˜“ç­†æ•¸)
    """
    # ç¬¬ä¸€å„ªå…ˆï¼šåŸºæ–¼å‚™æŸ¥ç·¨è™Ÿçµ±è¨ˆäº¤æ˜“ç­†æ•¸
    id_transaction_counts = transaction_df['å‚™æŸ¥ç·¨è™Ÿ'].value_counts().to_dict()
    
    # ç¬¬äºŒå‚™æ´ï¼šåŸºæ–¼è¤‡åˆéµçµ±è¨ˆäº¤æ˜“ç­†æ•¸ï¼ˆç”¨æ–¼æ²’æœ‰å‚™æŸ¥ç·¨è™ŸåŒ¹é…çš„æƒ…æ³ï¼‰
    composite_keys = (
        transaction_df[['ç¸£å¸‚', 'è¡Œæ”¿å€', 'ç¤¾å€åç¨±']]
        .fillna('')
        .astype(str)
        .agg('|'.join, axis=1)
    )
    
    # è¨ˆç®—æ¯å€‹ composite key çš„äº¤æ˜“ç­†æ•¸
    composite_key_counts = composite_keys.value_counts()
    
    return id_transaction_counts, composite_key_counts


def count_transactions_for_community(
    row: pd.Series, 
    id_transaction_counts: Dict[str, int],
    composite_key_counts: pd.Series
) -> int:
    """
    è¨ˆç®—å–®ä¸€ç¤¾å€çš„é å”®äº¤æ˜“ç­†æ•¸
    
    Parameters:
    -----------
    row : pd.Series
        ç¤¾å€è³‡æ–™çš„ä¸€åˆ—
    id_transaction_counts : Dict[str, int]
        å‚™æŸ¥ç·¨è™Ÿå°æ‡‰çš„äº¤æ˜“ç­†æ•¸
    composite_key_counts : pd.Series
        composite key å°æ‡‰çš„äº¤æ˜“ç­†æ•¸
        
    Returns:
    --------
    int
        è©²ç¤¾å€çš„é å”®äº¤æ˜“ç­†æ•¸
    """
    id_list = parse_id_string(row.get('å‚™æŸ¥ç·¨è™Ÿæ¸…å–®', ''))
    
    # æ–¹æ³•1ï¼šå„ªå…ˆä½¿ç”¨å‚™æŸ¥ç·¨è™Ÿçµ±è¨ˆ
    if id_list:
        # æª¢æŸ¥æ˜¯å¦æœ‰å‚™æŸ¥ç·¨è™Ÿèƒ½åœ¨çµ±è¨ˆä¸­æ‰¾åˆ°
        found_ids = [id for id in id_list if id in id_transaction_counts]
        
        if found_ids:
            # åŸºæ–¼å‚™æŸ¥ç·¨è™Ÿçµ±è¨ˆï¼Œç´¯åŠ æ‰€æœ‰åŒ¹é…çš„å‚™æŸ¥ç·¨è™Ÿäº¤æ˜“ç­†æ•¸
            return sum(id_transaction_counts[id] for id in found_ids)
    
    # æ–¹æ³•2ï¼šå‚™æŸ¥ç·¨è™Ÿç„¡æ³•åŒ¹é…æ™‚ï¼Œä½¿ç”¨ composite key
    composite_key = f"{row.get('ç¸£å¸‚', '')}|{row.get('è¡Œæ”¿å€', '')}|{row.get('ç¤¾å€åç¨±', '')}"
    return composite_key_counts.get(composite_key, 0)


def calculate_presale_transaction_counts(community_df: pd.DataFrame, transaction_df: pd.DataFrame) -> pd.Series:
    """
    è¨ˆç®—ç¤¾å€çš„é å”®äº¤æ˜“ç­†æ•¸
    
    Parameters:
    -----------
    community_df : pd.DataFrame
        åŒ…å«å‚™æŸ¥ç·¨è™Ÿæ¸…å–®çš„ç¤¾å€è³‡æ–™ï¼Œéœ€åŒ…å«ä»¥ä¸‹æ¬„ä½ï¼š
        - å‚™æŸ¥ç·¨è™Ÿæ¸…å–®: é€—è™Ÿåˆ†éš”çš„å‚™æŸ¥ç·¨è™Ÿå­—ä¸²
        - ç¸£å¸‚: ç¸£å¸‚åç¨±
        - è¡Œæ”¿å€: è¡Œæ”¿å€åç¨±  
        - ç¤¾å€åç¨±: ç¤¾å€åç¨±
        
    transaction_df : pd.DataFrame  
        äº¤æ˜“è³‡æ–™ï¼Œéœ€åŒ…å«ä»¥ä¸‹æ¬„ä½ï¼š
        - å‚™æŸ¥ç·¨è™Ÿ: å‚™æŸ¥ç·¨è™Ÿ
        - ç¸£å¸‚: ç¸£å¸‚åç¨±
        - è¡Œæ”¿å€: è¡Œæ”¿å€åç¨±
        - ç¤¾å€åç¨±: ç¤¾å€åç¨±
        
    Returns:
    --------
    pd.Series
        æ¯å€‹ç¤¾å€å°æ‡‰çš„é å”®äº¤æ˜“ç­†æ•¸ï¼Œç´¢å¼•èˆ‡ community_df ç›¸åŒ
        
    Raises:
    -------
    ValueError
        ç•¶å¿…è¦æ¬„ä½ç¼ºå¤±æ™‚
    """
    # æª¢æŸ¥å¿…è¦æ¬„ä½
    required_community_cols = ['å‚™æŸ¥ç·¨è™Ÿæ¸…å–®', 'ç¸£å¸‚', 'è¡Œæ”¿å€', 'ç¤¾å€åç¨±']
    required_transaction_cols = ['å‚™æŸ¥ç·¨è™Ÿ', 'ç¸£å¸‚', 'è¡Œæ”¿å€', 'ç¤¾å€åç¨±']
    
    missing_community_cols = [col for col in required_community_cols if col not in community_df.columns]
    missing_transaction_cols = [col for col in required_transaction_cols if col not in transaction_df.columns]
    
    if missing_community_cols:
        raise ValueError(f"community_df ç¼ºå°‘å¿…è¦æ¬„ä½: {missing_community_cols}")
    if missing_transaction_cols:
        raise ValueError(f"transaction_df ç¼ºå°‘å¿…è¦æ¬„ä½: {missing_transaction_cols}")
    
    # å»ºç«‹æŸ¥è©¢çµæ§‹
    id_transaction_counts, composite_key_counts = create_transaction_lookup_structures(transaction_df)
    
    # è¨ˆç®—æ¯å€‹ç¤¾å€çš„äº¤æ˜“ç­†æ•¸
    transaction_counts = community_df.apply(
        lambda row: count_transactions_for_community(
            row, id_transaction_counts, composite_key_counts
        ),
        axis=1
    )
    
    return transaction_counts


def calculate_cancellation_counts(community_df: pd.DataFrame, transaction_df: pd.DataFrame) -> pd.Series:
    """
    è¨ˆç®—ç¤¾å€çš„è§£ç´„æ¬¡æ•¸
    
    Parameters:
    -----------
    community_df : pd.DataFrame
        åŒ…å«å‚™æŸ¥ç·¨è™Ÿæ¸…å–®çš„ç¤¾å€è³‡æ–™ï¼Œéœ€åŒ…å«ä»¥ä¸‹æ¬„ä½ï¼š
        - å‚™æŸ¥ç·¨è™Ÿæ¸…å–®: é€—è™Ÿåˆ†éš”çš„å‚™æŸ¥ç·¨è™Ÿå­—ä¸²
        - ç¸£å¸‚: ç¸£å¸‚åç¨±
        - è¡Œæ”¿å€: è¡Œæ”¿å€åç¨±  
        - ç¤¾å€åç¨±: ç¤¾å€åç¨±
        
    transaction_df : pd.DataFrame  
        äº¤æ˜“è³‡æ–™ï¼Œéœ€åŒ…å«ä»¥ä¸‹æ¬„ä½ï¼š
        - å‚™æŸ¥ç·¨è™Ÿ: å‚™æŸ¥ç·¨è™Ÿ
        - ç¸£å¸‚: ç¸£å¸‚åç¨±
        - è¡Œæ”¿å€: è¡Œæ”¿å€åç¨±
        - ç¤¾å€åç¨±: ç¤¾å€åç¨±
        - è§£ç´„æƒ…å½¢: è§£ç´„ç‹€æ…‹è³‡è¨Š
        
    Returns:
    --------
    pd.Series
        æ¯å€‹ç¤¾å€å°æ‡‰çš„è§£ç´„æ¬¡æ•¸ï¼Œç´¢å¼•èˆ‡ community_df ç›¸åŒ
        
    Raises:
    -------
    ValueError
        ç•¶å¿…è¦æ¬„ä½ç¼ºå¤±æ™‚
    """
    # æª¢æŸ¥å¿…è¦æ¬„ä½
    required_community_cols = ['å‚™æŸ¥ç·¨è™Ÿæ¸…å–®', 'ç¸£å¸‚', 'è¡Œæ”¿å€', 'ç¤¾å€åç¨±']
    required_transaction_cols = ['å‚™æŸ¥ç·¨è™Ÿ', 'ç¸£å¸‚', 'è¡Œæ”¿å€', 'ç¤¾å€åç¨±', 'è§£ç´„æƒ…å½¢']
    
    missing_community_cols = [col for col in required_community_cols if col not in community_df.columns]
    missing_transaction_cols = [col for col in required_transaction_cols if col not in transaction_df.columns]
    
    if missing_community_cols:
        raise ValueError(f"community_df ç¼ºå°‘å¿…è¦æ¬„ä½: {missing_community_cols}")
    if missing_transaction_cols:
        raise ValueError(f"transaction_df ç¼ºå°‘å¿…è¦æ¬„ä½: {missing_transaction_cols}")
    
    # å»ºç«‹è§£ç´„æŸ¥è©¢çµæ§‹
    def create_cancellation_lookup_structures(transaction_df):
        # ç¬¬ä¸€å„ªå…ˆï¼šåŸºæ–¼å‚™æŸ¥ç·¨è™Ÿçµ±è¨ˆè§£ç´„ç­†æ•¸
        cancellation_df = transaction_df[transaction_df['è§£ç´„æƒ…å½¢'].notna()]
        id_cancellation_counts = cancellation_df['å‚™æŸ¥ç·¨è™Ÿ'].value_counts().to_dict()
        
        # ç¬¬äºŒå‚™æ´ï¼šåŸºæ–¼è¤‡åˆéµçµ±è¨ˆè§£ç´„ç­†æ•¸
        composite_keys = (
            cancellation_df[['ç¸£å¸‚', 'è¡Œæ”¿å€', 'ç¤¾å€åç¨±']]
            .fillna('')
            .astype(str)
            .agg('|'.join, axis=1)
        )
        composite_cancellation_counts = composite_keys.value_counts()
        
        return id_cancellation_counts, composite_cancellation_counts
    
    def count_cancellations_for_community(row, id_cancellation_counts, composite_cancellation_counts):
        id_list = parse_id_string(row.get('å‚™æŸ¥ç·¨è™Ÿæ¸…å–®', ''))
        
        # æ–¹æ³•1ï¼šå„ªå…ˆä½¿ç”¨å‚™æŸ¥ç·¨è™Ÿçµ±è¨ˆ
        if id_list:
            found_ids = [id for id in id_list if id in id_cancellation_counts]
            if found_ids:
                return sum(id_cancellation_counts[id] for id in found_ids)
        
        # æ–¹æ³•2ï¼šå‚™æŸ¥ç·¨è™Ÿç„¡æ³•åŒ¹é…æ™‚ï¼Œä½¿ç”¨ composite key
        composite_key = f"{row.get('ç¸£å¸‚', '')}|{row.get('è¡Œæ”¿å€', '')}|{row.get('ç¤¾å€åç¨±', '')}"
        return composite_cancellation_counts.get(composite_key, 0)
    
    # å»ºç«‹æŸ¥è©¢çµæ§‹
    id_cancellation_counts, composite_cancellation_counts = create_cancellation_lookup_structures(transaction_df)
    
    # è¨ˆç®—æ¯å€‹ç¤¾å€çš„è§£ç´„æ¬¡æ•¸
    cancellation_counts = community_df.apply(
        lambda row: count_cancellations_for_community(
            row, id_cancellation_counts, composite_cancellation_counts
        ),
        axis=1
    )
    
    return cancellation_counts


def create_fast_transaction_lookup(transaction_df: pd.DataFrame) -> tuple:
    """
    å»ºç«‹å¿«é€ŸæŸ¥è©¢çµæ§‹ï¼Œé¿å…é‡è¤‡éæ¿¾å’ŒæŸ¥è©¢
    
    Returns:
    --------
    tuple: (id_first_dates, composite_first_dates)
        - id_first_dates: å‚™æŸ¥ç·¨è™Ÿå°æ‡‰çš„æœ€åˆäº¤æ˜“æ—¥æœŸå­—å…¸
        - composite_first_dates: è¤‡åˆéµå°æ‡‰çš„æœ€åˆäº¤æ˜“æ—¥æœŸå­—å…¸
    """
    print("ğŸ”§ å»ºç«‹å¿«é€ŸæŸ¥è©¢çµæ§‹...")
    
    # ä¸€æ¬¡æ€§éæ¿¾æ­£å¸¸äº¤æ˜“ï¼ˆéè§£ç´„ï¼‰
    normal_transactions = transaction_df[transaction_df['è§£ç´„æƒ…å½¢'].isna()].copy()
    print(f"   æ­£å¸¸äº¤æ˜“ç­†æ•¸ï¼š{len(normal_transactions):,}")
    
    # æ–¹æ³•1ï¼šåŸºæ–¼å‚™æŸ¥ç·¨è™Ÿçš„æœ€åˆäº¤æ˜“æ—¥æœŸæŸ¥è©¢è¡¨
    id_first_dates = {}
    if 'å‚™æŸ¥ç·¨è™Ÿ' in normal_transactions.columns:
        # ç§»é™¤ç©ºå€¼çš„å‚™æŸ¥ç·¨è™Ÿ
        valid_id_transactions = normal_transactions.dropna(subset=['å‚™æŸ¥ç·¨è™Ÿ'])
        if not valid_id_transactions.empty:
            # ä½¿ç”¨ groupby ä¸€æ¬¡æ€§è¨ˆç®—æ‰€æœ‰å‚™æŸ¥ç·¨è™Ÿçš„æœ€åˆæ—¥æœŸ
            id_groups = valid_id_transactions.groupby('å‚™æŸ¥ç·¨è™Ÿ')['äº¤æ˜“æ—¥æœŸ'].min()
            id_first_dates = id_groups.to_dict()
    
    print(f"   å‚™æŸ¥ç·¨è™ŸæŸ¥è©¢è¡¨ï¼š{len(id_first_dates):,} ç­†")
    
    # æ–¹æ³•2ï¼šåŸºæ–¼è¤‡åˆéµçš„æœ€åˆäº¤æ˜“æ—¥æœŸæŸ¥è©¢è¡¨
    composite_first_dates = {}
    if not normal_transactions.empty:
        # å»ºç«‹è¤‡åˆéµ
        normal_transactions['composite_key'] = (
            normal_transactions['ç¸£å¸‚'].fillna('') + '|' +
            normal_transactions['è¡Œæ”¿å€'].fillna('') + '|' +
            normal_transactions['ç¤¾å€åç¨±'].fillna('')
        )
        
        # ä½¿ç”¨ groupby ä¸€æ¬¡æ€§è¨ˆç®—æ‰€æœ‰è¤‡åˆéµçš„æœ€åˆæ—¥æœŸ
        composite_groups = normal_transactions.groupby('composite_key')['äº¤æ˜“æ—¥æœŸ'].min()
        composite_first_dates = composite_groups.to_dict()
    
    print(f"   è¤‡åˆéµæŸ¥è©¢è¡¨ï¼š{len(composite_first_dates):,} ç­†")
    print("âœ… æŸ¥è©¢çµæ§‹å»ºç«‹å®Œæˆ")
    
    return id_first_dates, composite_first_dates


def find_first_transaction_date_fast(
    row: pd.Series, 
    id_first_dates: dict,
    composite_first_dates: dict
) -> pd.Timestamp:
    """
    ä½¿ç”¨é å»ºæŸ¥è©¢è¡¨å¿«é€Ÿæ‰¾å‡ºç¤¾å€æœ€åˆäº¤æ˜“æ—¥æœŸ
    
    Parameters:
    -----------
    row : pd.Series
        ç¤¾å€è³‡æ–™çš„ä¸€åˆ—
    id_first_dates : dict
        å‚™æŸ¥ç·¨è™Ÿå°æ‡‰æœ€åˆäº¤æ˜“æ—¥æœŸçš„å­—å…¸
    composite_first_dates : dict
        è¤‡åˆéµå°æ‡‰æœ€åˆäº¤æ˜“æ—¥æœŸçš„å­—å…¸
        
    Returns:
    --------
    pd.Timestamp or pd.NaT
        è©²ç¤¾å€çš„æœ€åˆäº¤æ˜“æ—¥æœŸ
    """
    # æ–¹æ³•1ï¼šå„ªå…ˆä½¿ç”¨å‚™æŸ¥ç·¨è™ŸåŒ¹é…
    id_list = parse_id_string(row.get('å‚™æŸ¥ç·¨è™Ÿæ¸…å–®', ''))
    
    if id_list:
        # æ‰¾å‡ºæ‰€æœ‰åŒ¹é…çš„å‚™æŸ¥ç·¨è™Ÿå°æ‡‰çš„äº¤æ˜“æ—¥æœŸ
        matched_dates = []
        for backup_id in id_list:
            if backup_id in id_first_dates:
                matched_dates.append(id_first_dates[backup_id])
        
        if matched_dates:
            return min(matched_dates)  # è¿”å›æœ€æ—©çš„æ—¥æœŸ
    
    # æ–¹æ³•2ï¼šä½¿ç”¨è¤‡åˆéµåŒ¹é…
    composite_key = f"{row.get('ç¸£å¸‚', '')}|{row.get('è¡Œæ”¿å€', '')}|{row.get('ç¤¾å€åç¨±', '')}"
    
    if composite_key in composite_first_dates:
        return composite_first_dates[composite_key]
    
    # å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œè¿”å› NaT
    return pd.NaT


def calculate_first_transaction_dates_fast(
    community_df: pd.DataFrame, 
    transaction_df: pd.DataFrame
) -> pd.Series:
    """
    é«˜æ•ˆèƒ½ç‰ˆæœ¬ï¼šè¨ˆç®—æ‰€æœ‰ç¤¾å€çš„æœ€åˆäº¤æ˜“æ—¥æœŸ
    
    Parameters:
    -----------
    community_df : pd.DataFrame
        ç¤¾å€è³‡æ–™
    transaction_df : pd.DataFrame
        äº¤æ˜“è³‡æ–™
        
    Returns:
    --------
    pd.Series
        æ¯å€‹ç¤¾å€å°æ‡‰çš„æœ€åˆäº¤æ˜“æ—¥æœŸ
    """
    start_time = time.time()
    
    # æª¢æŸ¥å¿…è¦æ¬„ä½
    required_community_cols = ['å‚™æŸ¥ç·¨è™Ÿæ¸…å–®', 'ç¸£å¸‚', 'è¡Œæ”¿å€', 'ç¤¾å€åç¨±']
    required_transaction_cols = ['å‚™æŸ¥ç·¨è™Ÿ', 'äº¤æ˜“æ—¥æœŸ', 'è§£ç´„æƒ…å½¢']
    
    missing_community_cols = [col for col in required_community_cols if col not in community_df.columns]
    missing_transaction_cols = [col for col in required_transaction_cols if col not in transaction_df.columns]
    
    if missing_community_cols:
        raise ValueError(f"community_df ç¼ºå°‘å¿…è¦æ¬„ä½: {missing_community_cols}")
    if missing_transaction_cols:
        raise ValueError(f"transaction_df ç¼ºå°‘å¿…è¦æ¬„ä½: {missing_transaction_cols}")
    
    # å»ºç«‹å¿«é€ŸæŸ¥è©¢çµæ§‹
    id_first_dates, composite_first_dates = create_fast_transaction_lookup(transaction_df)
    
    # ä½¿ç”¨å‘é‡åŒ–æ“ä½œè¨ˆç®—æœ€åˆäº¤æ˜“æ—¥æœŸ
    print("ğŸš€ é–‹å§‹è¨ˆç®—æœ€åˆäº¤æ˜“æ—¥æœŸ...")
    
    # é¡¯ç¤ºé€²åº¦æ¢
    tqdm.pandas(desc="è¨ˆç®—é€²åº¦")
    
    first_dates = community_df.progress_apply(
        lambda row: find_first_transaction_date_fast(row, id_first_dates, composite_first_dates),
        axis=1
    )
    
    elapsed_time = time.time() - start_time
    print(f"âš¡ è¨ˆç®—å®Œæˆï¼è€—æ™‚ï¼š{elapsed_time:.2f} ç§’")
    
    return first_dates


def correct_sales_start_date(df, sales_col='éŠ·å”®èµ·å§‹æ™‚é–“', first_tx_col='æœ€åˆäº¤æ˜“æ—¥æœŸ'):
    """
    ä¿®æ­£éŠ·å”®èµ·å§‹æ™‚é–“ > æœ€åˆäº¤æ˜“æ—¥æœŸçš„æƒ…æ³ï¼Œåƒ…ç•¶å…©è€…çš†é NaT æ™‚é€²è¡Œæ›¿æ›ã€‚

    åƒæ•¸:
        df (pd.DataFrame): å«æœ‰æ—¥æœŸæ¬„ä½çš„ DataFrameã€‚
        sales_col (str): éŠ·å”®èµ·å§‹æ™‚é–“æ¬„ä½åç¨±ï¼Œé è¨­ç‚º 'éŠ·å”®èµ·å§‹æ™‚é–“'ã€‚
        first_tx_col (str): æœ€åˆäº¤æ˜“æ—¥æœŸæ¬„ä½åç¨±ï¼Œé è¨­ç‚º 'æœ€åˆäº¤æ˜“æ—¥æœŸ'ã€‚

    å›å‚³:
        ä¿®æ­£å¾Œçš„ DataFrameã€‚
    """
    # å»ºç«‹æ¢ä»¶
    mask = (
        df[sales_col].notna() &
        df[first_tx_col].notna() &
        (df[sales_col] > df[first_tx_col])
    )

    # é¡¯ç¤ºä¿®æ­£ç­†æ•¸
    print(f"ğŸ” å…±ä¿®æ­£ {mask.sum()} ç­†éŠ·å”®èµ·å§‹æ™‚é–“")

    # åŸ·è¡Œæ›¿ä»£
    df.loc[mask, sales_col] = df.loc[mask, first_tx_col]
    
    return df


# è™•ç†é‡è¤‡ç¤¾å€
def process_duplicate_communities(df):
    """
    è™•ç†é‡è¤‡ç¤¾å€çš„ä¸»å‡½æ•¸ v2.1
    
    è­˜åˆ¥æ¢ä»¶ï¼šè¡Œæ”¿å€ç›¸åŒ + å»ºç…§åŸ·ç…§ç›¸åŒ + ç¶“åº¦ç›¸åŒ
    è™•ç†è¦å‰‡ï¼š
    1. æ˜¥è±†å­æ¡ˆä¾‹ï¼šæˆ¶æ•¸ç›¸åŠ 
    2. ä¸€èˆ¬é‡è¤‡ï¼šå–éŠ·å”®èµ·å§‹æ™‚é–“è¼ƒæ™šçš„è¨˜éŒ„
    
    é—œé€£ç·¨è™Ÿé‚è¼¯ (v2.1ä¿®æ­£ç‰ˆ)ï¼š
    - ç¤¾å€æœ‰æ•ˆæ€§æ¬„ä½ï¼š1(æœ‰æ•ˆ)/0(ç„¡æ•ˆ)
    - é—œé€£ç·¨è™Ÿæ¬„ä½ï¼šæœ‰æ•ˆç¤¾å€=è¢«åˆä½µçš„ç„¡æ•ˆç¤¾å€ç·¨è™Ÿåˆ—è¡¨ / ç„¡æ•ˆç¤¾å€=ç©ºå€¼
    
    è¿”å›ï¼š
    - processed_df: è™•ç†å¾Œçš„DataFrameï¼ˆåŒ…å«æ–°å¢æ¬„ä½ï¼‰
    - report: è™•ç†å ±å‘Šï¼ˆåŒ…å«é—œé€£ç·¨è™Ÿå®Œæ•´æ€§æª¢æŸ¥ï¼‰
    """
    
    # å‰µå»ºå·¥ä½œå‰¯æœ¬
    df_processed = df.copy()
    
    # æ–°å¢ç¤¾å€æœ‰æ•ˆæ€§æ¬„ä½å’Œé—œé€£ç·¨è™Ÿæ¬„ä½ï¼ˆé è¨­ç‚ºæœ‰æ•ˆï¼‰
    df_processed['ç¤¾å€æœ‰æ•ˆæ€§'] = 1
    df_processed['é—œé€£ç·¨è™Ÿ'] = None
    
    # æ­¥é©Ÿ1ï¼šè­˜åˆ¥é‡è¤‡ç¤¾å€ç¾¤çµ„
    print("ğŸ” è­˜åˆ¥é‡è¤‡ç¤¾å€ç¾¤çµ„...")
    duplicate_groups = identify_duplicate_groups(df_processed)
    
    # æ­¥é©Ÿ2ï¼šè™•ç†æ¯å€‹é‡è¤‡ç¾¤çµ„
    print("âš™ï¸ è™•ç†é‡è¤‡ç¤¾å€...")
    process_results = []
    
    for group_id, group_indices in duplicate_groups.items():
        if len(group_indices) > 1:  # åªè™•ç†çœŸæ­£çš„é‡è¤‡ç¾¤çµ„
            group_data = df_processed.loc[group_indices]
            result = process_single_group(df_processed, group_indices, group_data)
            process_results.append(result)
    

    
    return df_processed

def identify_duplicate_groups(df):
    """
    è­˜åˆ¥é‡è¤‡ç¤¾å€ç¾¤çµ„
    æ¢ä»¶ï¼šè¡Œæ”¿å€ + å»ºç…§åŸ·ç…§ + ç¶“åº¦ç›¸åŒ
    """
    duplicate_groups = {}
    group_id = 0
    
    # å»ºç«‹åˆ†çµ„æ¢ä»¶
    df['group_key'] = df['è¡Œæ”¿å€'].astype(str) + '|' + \
                     df['å»ºç…§åŸ·ç…§'].astype(str) + '|' + \
                     df['ç¶“åº¦'].astype(str)
    
    # æ‰¾å‡ºé‡è¤‡ç¾¤çµ„
    group_counts = df['group_key'].value_counts()
    duplicate_keys = group_counts[group_counts > 1].index
    
    for key in duplicate_keys:
        indices = df[df['group_key'] == key].index.tolist()
        if len(indices) > 1:
            duplicate_groups[group_id] = indices
            group_id += 1
    
    # æ¸…ç†è‡¨æ™‚æ¬„ä½
    df.drop('group_key', axis=1, inplace=True)
    
    print(f"ç™¼ç¾ {len(duplicate_groups)} å€‹é‡è¤‡ç¾¤çµ„")
    return duplicate_groups

def process_single_group(df, group_indices, group_data):
    """
    è™•ç†å–®ä¸€é‡è¤‡ç¾¤çµ„
    """
    group_info = {
        'group_indices': group_indices,
        'communities': group_data[['ç·¨è™Ÿ', 'ç¤¾å€åç¨±', 'éŠ·å”®èµ·å§‹æ™‚é–“', 'æˆ¶æ•¸', 'é å”®äº¤æ˜“ç­†æ•¸', 'è§£ç´„ç­†æ•¸']].to_dict('records'),
        'processing_type': '',
        'valid_community': None,
        'invalid_communities': [],
        'merged_data': {}
    }
    
    # æª¢æŸ¥æ˜¯å¦ç‚ºæ˜¥è±†å­ç‰¹æ®Šæ¡ˆä¾‹
    if is_spring_bean_case(group_data):
        result = process_spring_bean_case(df, group_indices, group_data)
        group_info.update(result)
        group_info['processing_type'] = 'æ˜¥è±†å­ç‰¹ä¾‹ï¼ˆæˆ¶æ•¸ç›¸åŠ ï¼‰'
    else:
        result = process_general_case(df, group_indices, group_data)
        group_info.update(result)
        group_info['processing_type'] = 'ä¸€èˆ¬é‡è¤‡ï¼ˆå–è¼ƒæ™šéŠ·å”®æ—¥æœŸï¼‰'
    
    return group_info

def is_spring_bean_case(group_data):
    """
    åˆ¤æ–·æ˜¯å¦ç‚ºæ˜¥è±†å­æ¡ˆä¾‹
    """
    community_names = group_data['ç¤¾å€åç¨±'].tolist()
    return ('æ˜¥è±†å­-å…¬å¯“' in community_names and 'æ˜¥è±†å­-é€å¤©' in community_names)

def process_spring_bean_case(df, group_indices, group_data):
    """
    è™•ç†æ˜¥è±†å­ç‰¹æ®Šæ¡ˆä¾‹ï¼šæˆ¶æ•¸ç›¸åŠ 
    """
    # é¸æ“‡ç¬¬ä¸€å€‹è¨˜éŒ„ä½œç‚ºæœ‰æ•ˆç¤¾å€
    valid_idx = group_indices[0]
    invalid_indices = group_indices[1:]
    valid_code = df.loc[valid_idx, 'ç·¨è™Ÿ']
    
    # è¨ˆç®—åˆä½µå¾Œçš„æ•¸å€¼
    total_units = group_data['æˆ¶æ•¸'].sum()
    total_transactions = group_data['é å”®äº¤æ˜“ç­†æ•¸'].sum()
    total_cancellations = group_data['è§£ç´„ç­†æ•¸'].sum()
    
    # åˆä½µå‚™æŸ¥ç·¨è™Ÿ
    backup_numbers = group_data['å‚™æŸ¥ç·¨è™Ÿæ¸…å–®'].dropna().tolist()
    merged_backup_numbers = ', '.join(backup_numbers)
    
    # æ”¶é›†ç„¡æ•ˆç¤¾å€ç·¨è™Ÿ
    invalid_codes = [df.loc[idx, 'ç·¨è™Ÿ'] for idx in invalid_indices]
    merged_invalid_codes = ', '.join(invalid_codes)
    
    # æ›´æ–°æœ‰æ•ˆç¤¾å€çš„æ•¸æ“š
    df.loc[valid_idx, 'æˆ¶æ•¸'] = total_units
    df.loc[valid_idx, 'é å”®äº¤æ˜“ç­†æ•¸'] = total_transactions
    df.loc[valid_idx, 'è§£ç´„ç­†æ•¸'] = total_cancellations
    df.loc[valid_idx, 'å‚™æŸ¥ç·¨è™Ÿæ¸…å–®'] = merged_backup_numbers
    df.loc[valid_idx, 'é—œé€£ç·¨è™Ÿ'] = merged_invalid_codes  # æœ‰æ•ˆç¤¾å€è¨˜éŒ„ç„¡æ•ˆç¤¾å€ç·¨è™Ÿ
    
    # æ¨™è¨˜ç„¡æ•ˆç¤¾å€ï¼Œé—œé€£ç·¨è™Ÿä¿æŒç‚ºç©º
    for idx in invalid_indices:
        df.loc[idx, 'ç¤¾å€æœ‰æ•ˆæ€§'] = 0
        df.loc[idx, 'é—œé€£ç·¨è™Ÿ'] = None  # ç„¡æ•ˆç¤¾å€é—œé€£ç·¨è™Ÿç‚ºç©º
    
    return {
        'valid_community': valid_idx,
        'valid_code': valid_code,
        'invalid_communities': invalid_indices,
        'invalid_codes': invalid_codes,
        'merged_data': {
            'æˆ¶æ•¸': total_units,
            'é å”®äº¤æ˜“ç­†æ•¸': total_transactions,
            'è§£ç´„ç­†æ•¸': total_cancellations,
            'å‚™æŸ¥ç·¨è™Ÿæ¸…å–®': merged_backup_numbers,
            'é—œé€£ç·¨è™Ÿ': merged_invalid_codes
        }
    }

def process_general_case(df, group_indices, group_data):
    """
    è™•ç†ä¸€èˆ¬é‡è¤‡æ¡ˆä¾‹ï¼šå–éŠ·å”®èµ·å§‹æ™‚é–“è¼ƒæ™šçš„è¨˜éŒ„
    """
    # è½‰æ›éŠ·å”®èµ·å§‹æ™‚é–“ç‚ºæ—¥æœŸæ ¼å¼é€²è¡Œæ¯”è¼ƒ
    group_data_copy = group_data.copy()
    group_data_copy['éŠ·å”®èµ·å§‹æ™‚é–“_date'] = pd.to_datetime(group_data_copy['éŠ·å”®èµ·å§‹æ™‚é–“'], errors='coerce')
    
    # æ‰¾å‡ºéŠ·å”®èµ·å§‹æ™‚é–“æœ€æ™šçš„è¨˜éŒ„ï¼Œå¦‚æœæ™‚é–“ç›¸åŒå‰‡é¸äº¤æ˜“ç­†æ•¸è¼ƒå¤šçš„
    valid_idx = None
    latest_date = group_data_copy['éŠ·å”®èµ·å§‹æ™‚é–“_date'].max()
    latest_records = group_data_copy[group_data_copy['éŠ·å”®èµ·å§‹æ™‚é–“_date'] == latest_date]
    
    if len(latest_records) == 1:
        valid_idx = latest_records.index[0]
    else:
        # æ™‚é–“ç›¸åŒæ™‚ï¼Œé¸æ“‡äº¤æ˜“ç­†æ•¸è¼ƒå¤šçš„
        valid_idx = latest_records['é å”®äº¤æ˜“ç­†æ•¸'].idxmax()
    
    invalid_indices = [idx for idx in group_indices if idx != valid_idx]
    valid_code = df.loc[valid_idx, 'ç·¨è™Ÿ']
    
    # è¨ˆç®—åˆä½µå¾Œçš„æ•¸å€¼
    total_transactions = group_data['é å”®äº¤æ˜“ç­†æ•¸'].sum()
    total_cancellations = group_data['è§£ç´„ç­†æ•¸'].sum()
    
    # åˆä½µå‚™æŸ¥ç·¨è™Ÿ
    backup_numbers = group_data['å‚™æŸ¥ç·¨è™Ÿæ¸…å–®'].dropna().tolist()
    merged_backup_numbers = ', '.join(backup_numbers)
    
    # æ”¶é›†ç„¡æ•ˆç¤¾å€ç·¨è™Ÿ
    invalid_codes = [df.loc[idx, 'ç·¨è™Ÿ'] for idx in invalid_indices]
    merged_invalid_codes = ', '.join(invalid_codes) if invalid_codes else None
    
    # æ›´æ–°æœ‰æ•ˆç¤¾å€çš„æ•¸æ“š
    df.loc[valid_idx, 'é å”®äº¤æ˜“ç­†æ•¸'] = total_transactions
    df.loc[valid_idx, 'è§£ç´„ç­†æ•¸'] = total_cancellations
    df.loc[valid_idx, 'å‚™æŸ¥ç·¨è™Ÿæ¸…å–®'] = merged_backup_numbers
    df.loc[valid_idx, 'é—œé€£ç·¨è™Ÿ'] = merged_invalid_codes  # æœ‰æ•ˆç¤¾å€è¨˜éŒ„ç„¡æ•ˆç¤¾å€ç·¨è™Ÿ
    # æˆ¶æ•¸ä¿æŒåŸå€¼ï¼ˆè¼ƒæ™šè¨˜éŒ„çš„æˆ¶æ•¸ï¼‰
    
    # æ¨™è¨˜ç„¡æ•ˆç¤¾å€ï¼Œé—œé€£ç·¨è™Ÿä¿æŒç‚ºç©º
    for idx in invalid_indices:
        df.loc[idx, 'ç¤¾å€æœ‰æ•ˆæ€§'] = 0
        df.loc[idx, 'é—œé€£ç·¨è™Ÿ'] = None  # ç„¡æ•ˆç¤¾å€é—œé€£ç·¨è™Ÿç‚ºç©º
    
    return {
        'valid_community': valid_idx,
        'valid_code': valid_code,
        'invalid_communities': invalid_indices,
        'invalid_codes': invalid_codes,
        'merged_data': {
            'æˆ¶æ•¸': df.loc[valid_idx, 'æˆ¶æ•¸'],  # ä¿æŒåŸå€¼
            'é å”®äº¤æ˜“ç­†æ•¸': total_transactions,
            'è§£ç´„ç­†æ•¸': total_cancellations,
            'å‚™æŸ¥ç·¨è™Ÿæ¸…å–®': merged_backup_numbers,
            'é—œé€£ç·¨è™Ÿ': merged_invalid_codes
        }
    }